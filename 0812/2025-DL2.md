## 1. Gradient descent 알고리즘 구현하기

**Gradient descent** 알고리즘은 **손실 함수(loss function)의 미분값**인 **gradient**를 이용해 모델에게 맞는 **최적의 가중치**(**weight**), 즉 손실 함수의 값을 최소화 하는 가중치를 구할 수 있는 알고리즘입니다.

이번 실습에서는 Gradient descent 알고리즘을 직접 구현한 후, 이를 이용해 데이터를 가장 잘 설명하는 **선형 회귀 직선의 기울기와 y절편**, 즉 선형 회귀 모델에게 맞는 최적의 가중치를 찾아보겠습니다.

선형 회귀 직선의 수식은 다음과 같은 1차 함수 형태이며, 우리가 Gradient descent 알고리즘을 사용해 찾을 값, 즉 가중치는 w0과 w1입니다.

f(x)=w0+w1x

------

### **손실 함수 (loss function)**

손실 함수(loss function)는 **실제값과 모델이 예측한 값 간의 차이**를 계산해주는 함수입니다. 손실 함수의 값은 가중치와 편향을 업데이트하는 데에 사용됩니다. 여기서는 손실 함수로 **MSE** (Mean Squared Error)를 사용합니다.

MSE는 **평균 제곱 오차** 함수로, 수식은 다음과 같습니다.

![image-20250312205520945](C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20250312205520945.png)

------

### **편미분**

gradient에는 **편미분**이라는 개념이 들어갑니다. 때문에 gradient를 설명하기 전 편미분에 대해 간단하게 짚고 넘어가겠습니다. 편미분이란 2개 이상의 변수를 가진 함수에서 우리가 미분할 하나의 변수를 제외한 나머지 변수들을 상수로 보고, 미분 대상인 그 변수로 미분하는 것입니다.

![image-20250312205603549](C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20250312205603549.png)

------

### **Gradient**

gradient는 곧 **기울기 벡터**를 의미하며, 선형 함수의 각 파라미터들의 편미분으로 구성된 열벡터로 정의합니다.

강의 자료 9페이지를 보면 학습률(learning rate)을 나타내는 α*α*가 있고, gradient를 나타내는 수식인 ▽Loss(W)가 있습니다. 즉 이를 풀어서 쓰면 다음과 같은 열벡터 형태입니다.

![image-20250312205642760](C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20250312205642760.png)

------

### **가중치 업데이트**

위와 같이 구한 w0와 w1의 gradient와 학습률 α를 이용해 가중치를 업데이트하는 공식은 다음과 같습니다.

![image-20250312205710869](C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20250312205710869.png)

## **실습**

1. 설명 중 ‘손실 함수’ 파트의 수식을 참고해 MSE 손실 함수를 완성하세요.
2. 설명 중 ‘Gradient’ 파트의 마지막 두 수식을 참고해 `w0`와 `w1`에 대한 gradient인 `gradient0`과 `gradient1`을 반환하는 함수를 완성하세요.
3. 설명 중 ‘가중치 업데이트’ 파트의 두 수식을 참고해 gradient descent를 통한 가중치 업데이트 코드를 작성하세요.