## 2. 역전파(Back propagation)

역전파(Back propagation)는 다층 퍼셉트론 모델을 이루는 가중치들을 개선하기 위해 개발된 여러 알고리즘들 중 가장 유명하고 널리 쓰이는 방법입니다.

이번 실습에서는 역전파를 간단하게 실습해보기 위해, 퍼셉트론 한 개의 가중치들을 개선시키는 역전파를 구현해 보도록 합니다.

------

다음 그림은 이번 실습에서 사용되는 퍼셉트론을 나타냅니다. 입력은 x1,x2,x3 세 개의 정수로 주어지고, 각각 w1,w2,w3의 계수가 곱해져 sigmoid 함수를 통과할 값은 x1w1+x2w2+x3w3가 됩니다.

여기서 w1,w2,w3가 바로 우리가 이번 실습에서 알아내야 하는 가중치입니다.

![image1](https://cdn-api.elice.io/api/archive/unzip_e78be103c8554fe6b34d0493689412d3/4ee84a82b5e4c9e6651b13fd27dcf615e427ec584929f2cef7167aa99151a77a/image.png?se=2024-11-11T00%3A15%3A00Z&sp=r&sv=2021-12-02&sr=b&sig=EIZ%2BCKZL5fZF8rZRgYO9oWuH0WPPZJhCGd%2Bukf4Ytco%3D)

x1w1+x2w2+x3w3가 sigmoid 함수를 거치고 나면 0 ~ 1 사이의 값으로 변환됩니다. 이는 특정 클래스로 분류될 확률을 나타내며, 0.5보다 작을 경우 0으로, 0.5보다 클 경우 1로 분류된다고 합시다.

------

이제 이 퍼셉트론을 학습시키려고 합니다. 좀 더 정확히 이야기하면, x1,x2,x3와 그 클래스 y가 여러 개 주어질 때, y값을 가장 잘 예측하는 w1,w2,w3를 찾아야 합니다.

예를 들어, 우리가 갖고 있는 훈련용 데이터가 다음과 같이 3개로 주어진다고 합시다.

- (1, 0, 0) –> 0
- (1, 0, 1) –> 1
- (0, 0, 1) –> 1

그렇다면 w1 = 0, w2 = 0, w3 = 1 이어야 함을 알 수 있습니다.

------

물론 이와 같은 최적의 w1, w2, w3 값을 처음부터 알 수는 없습니다. 따라서 우선 가중치 w\들을 초기화하고, 이를 여러 번의 학습을 거쳐 알아내야 합니다.

즉, 손실 함수(loss function)의 gradient 값을 역전파해서 받은 후, 그 값을 참고하여 손실 함수값을 최소화 하는 방향으로 w1,w2,w3를 업데이트 합니다.

이때, w1,w2,w3이 잘 개선되서 더 업데이트해도 변화가 거의 없을 때까지 하는 것이 중요합니다.

## 실습

코드의 주석 설명을 따라서 `getParameters(X, y)` 함수를 완성하세요. 여기서 `X`와 `y`는 훈련용 데이터입니다. 필요하다면 설명의 굵은 글씨 부분을 참고하세요.

1. `X`의 한 원소가 3개이므로 가중치도 3개가 있어야 합니다. 초기 가중치 `w`를 [1,1,1]로 정의하는 코드를 작성하세요.
2. 초기 가중치 `w`를 모델에 맞게 계속 업데이트 해야합니다. 업데이트를 위해 초기 가중치 `w`에 더해지는 값들의 리스트 `wPrime`을 [0,0,0]로 정의하는 코드를 작성하세요.
3. sigmoid 함수를 통과할 `r`값과 sigmoid 함수를 통과한 `r`값인 `v`를 정의하세요.
4. 가중치 `w`가 더이상 업데이트가 안될 때까지 업데이트해주는 코드를 작성하세요. 자세한 내용은 코드 주석을 참고하세요.

더 나아가 여러 예제를 테스트 해 보면서 하나의 퍼셉트론이 100% 예측할 수 없는 훈련용 데이터는 어떤 것이 있는지 생각해보세요.

이는 1장에서 배웠듯이 왜 다층 퍼셉트론 모델이 데이터 분류에서 필요한지 알게 해줍니다.